{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from fastai.conv_learner import *\n",
    "from fastai.dataset import *\n",
    "from fastai.models.resnet import vgg_resnet50\n",
    "\n",
    "import json\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3.1.post2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "try:\n",
    "    from itertools import  ifilterfalse\n",
    "except ImportError: # py3k\n",
    "    from itertools import  filterfalse\n",
    "    \n",
    "\n",
    "class cust_loss(torch.nn.Module):  \n",
    "    def __init__(self):\n",
    "        super(cust_loss, self).__init__()\n",
    "        \n",
    "    def forward(self, input,target):\n",
    "       # print(type(input), type(target))\n",
    "        loss = self.lovasz_hinge(input, target)\n",
    "        return loss\n",
    "        \n",
    "    def lovasz_grad(self,gt_sorted):\n",
    "        \"\"\"\n",
    "        Computes gradient of the Lovasz extension w.r.t sorted errors\n",
    "        See Alg. 1 in paper\n",
    "        \"\"\"\n",
    "        p = len(gt_sorted)\n",
    "        gts = gt_sorted.sum()\n",
    "        intersection = gts - gt_sorted.float().cumsum(0)\n",
    "        union = gts + (1 - gt_sorted).float().cumsum(0)\n",
    "        jaccard = 1. - intersection / union\n",
    "        if p > 1: # cover 1-pixel case\n",
    "            jaccard[1:p] = jaccard[1:p] - jaccard[0:-1]\n",
    "        return jaccard\n",
    "\n",
    "    def lovasz_hinge(self,logits, labels, per_image=True, ignore=None):\n",
    "        \"\"\"\n",
    "        Binary Lovasz hinge loss\n",
    "          logits: [B, H, W] Variable, logits at each pixel (between -\\infty and +\\infty)\n",
    "          labels: [B, H, W] Tensor, binary ground truth masks (0 or 1)\n",
    "          per_image: compute the loss per image instead of per batch\n",
    "          ignore: void class id\n",
    "        \"\"\"\n",
    "        #if per_image:\n",
    "      #  pdb.set_trace()\n",
    "        loss = self.mean(self.lovasz_hinge_flat(*self.flatten_binary_scores(log.unsqueeze(0), lab.unsqueeze(0), ignore))\n",
    "                              for log, lab in zip(logits, labels))\n",
    "#         else:\n",
    "#             loss = lovasz_hinge_flat(*flatten_binary_scores(logits, labels, ignore))\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def lovasz_hinge_flat(self,logits, labels):\n",
    "        \"\"\"\n",
    "        Binary Lovasz hinge loss\n",
    "          logits: [P] Variable, logits at each prediction (between -\\infty and +\\infty)\n",
    "          labels: [P] Tensor, binary ground truth labels (0 or 1)\n",
    "          ignore: label to ignore\n",
    "        \"\"\"\n",
    "        if len(labels) == 0:\n",
    "            # only void pixels, the gradients should be 0\n",
    "            return logits.sum() * 0.\n",
    "        signs = 2. * labels.float() - 1.\n",
    "        errors = (1. - logits * (signs))\n",
    "        errors_sorted, perm = torch.sort(errors, dim=0, descending=True)\n",
    "        perm = perm.data\n",
    "        gt_sorted = labels[perm]\n",
    "        grad = self.lovasz_grad(gt_sorted)\n",
    "        loss = torch.dot(F.elu(errors_sorted)+1, (grad))\n",
    "        return loss\n",
    "    \n",
    "    def flatten_binary_scores(self,scores, labels, ignore=None):\n",
    "        \"\"\"\n",
    "        Flattens predictions in the batch (binary case)\n",
    "        Remove labels equal to 'ignore'\n",
    "        \"\"\"\n",
    "        scores = scores.view(-1)\n",
    "        labels = labels.view(-1)\n",
    "        if ignore is None:\n",
    "            return scores, labels\n",
    "        valid = (labels != ignore)\n",
    "        vscores = scores[valid]\n",
    "        vlabels = labels[valid]\n",
    "        return vscores, vlabels\n",
    "    def mean(self,l, ignore_nan=False, empty=0):\n",
    "        \"\"\"\n",
    "        nanmean compatible with generators.\n",
    "        \"\"\"\n",
    "        l = iter(l)\n",
    "        if ignore_nan:\n",
    "            l = ifilterfalse(np.isnan, l)\n",
    "        try:\n",
    "            n = 1\n",
    "            acc = next(l)\n",
    "        except StopIteration:\n",
    "            if empty == 'raise':\n",
    "                raise ValueError('Empty mean')\n",
    "            return empty\n",
    "        for n, v in enumerate(l, 2):\n",
    "            acc += v\n",
    "        if n == 1:\n",
    "            return acc\n",
    "        return acc / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = Path('D:/data/TGS')\n",
    "MASKS_FN = 'train.csv'\n",
    "META_FN = 'metadata.csv'\n",
    "masks_csv = pd.read_csv(PATH/MASKS_FN)\n",
    "#meta_csv = pd.read_csv(PATH/META_FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_img(im, figsize=None, ax=None, alpha=None):\n",
    "    if not ax: fig,ax = plt.subplots(figsize=figsize)\n",
    "    ax.imshow(im, alpha=alpha)\n",
    "    ax.set_axis_off()\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DN = 'train-128'\n",
    "MASKS_DN = 'train_masks-128'\n",
    "sz = 128\n",
    "bs = 64\n",
    "nw = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DN = PATH/'train/images'\n",
    "MASKS_DN = PATH/'train/masks'\n",
    "test = PATH/'test/images'\n",
    "sz = 128\n",
    "bs = 40\n",
    "nw = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatchedFilesDataset(FilesDataset):\n",
    "    def __init__(self, fnames, y, transform, path):\n",
    "        self.y=y\n",
    "        assert((len(fnames))==len(y))\n",
    "        super().__init__(fnames, transform, path)\n",
    "    def get_y(self, i): return open_image(os.path.join(self.path, self.y[i]))\n",
    "    def get_c(self): return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x_names = np.array([Path(TRAIN_DN)/f'{o}.png' for o in masks_csv['id']])\n",
    "y_names = np.array([Path(MASKS_DN)/f'{o}.png' for o in masks_csv['id']])\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4000,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_names.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x_names = np.array([Path(TRAIN_DN)/f'{o}.png' for o in masks_csv['id']])\n",
    "# y_names = np.array([Path(MASKS_DN)/f'{o}.png' for o in masks_csv['id']])\n",
    "t_names=np.array([Path(test)/f'{o}' for o in os.listdir(test)])\n",
    "# t_names.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_idxs = list(range(500))\n",
    "((val_x,trn_x),(val_y,trn_y)) = split_by_idx(val_idxs, x_names, y_names)\n",
    "trn_x.shape\n",
    "trn_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "aug_tfms = [RandomFlip(tfm_y=TfmType.CLASS)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfms = tfms_from_model(resnet34, sz, crop_type=CropType.NO, tfm_y=TfmType.CLASS, aug_tfms=aug_tfms)\n",
    "datasets = ImageData.get_ds(MatchedFilesDataset, (trn_x,trn_y), (val_x,val_y), tfms,test=(t_names,t_names) ,path=PATH)\n",
    "md = ImageData(PATH, datasets, bs, num_workers=16, classes=None)\n",
    "denorm = md.trn_ds.denorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(md.trn_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([40, 3, 128, 128]), torch.Size([40, 128, 128]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape,y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "       ...          â‹±          ...       \n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "    0     0     0  ...      0     0     0\n",
       "[torch.cuda.FloatTensor of size 128x128 (GPU 0)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y[0][:][:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Simple upsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "f = resnet34\n",
    "cut,lr_cut = model_meta[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_base():\n",
    "    layers = cut_model(f(True), cut)\n",
    "    return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def IOU(pred, targs):\n",
    "    pred = (pred>0).float()\n",
    "    dice =  (pred*targs).sum() / (pred+targs).sum()\n",
    "    return dice\n",
    "\n",
    "def IOU(pred, targ):\n",
    "    print(type(pred, targ)\n",
    "    pred = (pred>0).float()\n",
    "    dice = np.sum((np.logical_and(pred, targ)+0.0001) / (np.logical_or(pred, targ))+0.0001)\n",
    "    return dice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class StdUpsample(nn.Module):\n",
    "    def __init__(self, nin, nout):\n",
    "        super().__init__()\n",
    "        self.conv = nn.ConvTranspose2d(nin, nout, 2, stride=2)\n",
    "        self.bn = nn.BatchNorm2d(nout)\n",
    "        \n",
    "    def forward(self, x): return self.bn(F.relu(self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class Upsample34(nn.Module):\n",
    "    def __init__(self, rn):\n",
    "        super().__init__()\n",
    "        self.rn = rn\n",
    "        self.features = nn.Sequential(\n",
    "            rn, nn.ReLU(),\n",
    "            StdUpsample(512,256),\n",
    "            StdUpsample(256,256),\n",
    "            StdUpsample(256,256),\n",
    "            StdUpsample(256,256),\n",
    "            nn.ConvTranspose2d(256, 1, 2, stride=2))\n",
    "        \n",
    "    def forward(self,x): return self.features(x)[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "class UpsampleModel():\n",
    "    def __init__(self,model,name='upsample'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n",
    "        return lgs + [children(self.model.features)[1:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m_base = get_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "m = to_gpu(Upsample34(m_base))\n",
    "models = UpsampleModel(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn = ConvLearner(md, models)\n",
    "learn.opt_fn=optim.SGD\n",
    "learn.crit=nn.BCEWithLogitsLoss()\n",
    "learn.metrics=[accuracy_thresh(0.5),IOU]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e057a4ac40649b684f3ffccacde5450",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>   IOU                                                                        \n",
      "    0      0.609377   89.381257  0.603043   0.07247   \n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn.lr_find()\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "lr=4e-2\n",
    "wd=1e-7\n",
    "lrs = np.array([lr/100,lr/10,lr])/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "294460b8ba364bc19c64d15f3bc378af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>   IOU                                                                        \n",
      "    0      0.631941   0.594411   0.756048   0.030099  \n",
      "    1      0.521956   0.423649   0.835174   0.302796                                                                   \n",
      "    2      0.394405   0.33395    0.866977   0.346353                                                                   \n",
      "    3      0.323748   0.30071    0.887075   0.379099                                                                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.30071]), 0.887075068950653, 0.3790991242327289]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lr,1, wds=wd, cycle_len=4,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save('tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.load('tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.bn_freeze(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a70bfea04cd4306a84f2e68c89b839d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=4), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>   IOU                                                                        \n",
      "    0      0.274593   0.275929   0.889948   0.377244  \n",
      "    1      0.249531   0.258714   0.897569   0.388394                                                                   \n",
      "    2      0.239902   0.252504   0.901776   0.394581                                                                   \n",
      "    3      0.226894   0.249679   0.901915   0.394869                                                                   \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([0.24968]), 0.901914918422699, 0.3948686790502473]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learn.fit(lrs,1,cycle_len=4,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "learn.save('128')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "x,y = next(iter(md.val_dl))\n",
    "py = to_np(learn.model(V(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## U-net (ish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveFeatures():\n",
    "    features=None\n",
    "    def __init__(self, m): self.hook = m.register_forward_hook(self.hook_fn)\n",
    "    def hook_fn(self, module, input, output): self.features = output\n",
    "    def remove(self): self.hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetBlock(nn.Module):\n",
    "    def __init__(self, up_in, x_in, n_out):\n",
    "        super().__init__()\n",
    "        up_out = x_out = n_out//2\n",
    "        self.x_conv  = nn.Conv2d(x_in,  x_out,  1)\n",
    "        self.tr_conv = nn.ConvTranspose2d(up_in, up_out, 2, stride=2)\n",
    "     #   self.drop = nn.Dropout2d(p=0.5)\n",
    "        self.bn = nn.BatchNorm2d(n_out)\n",
    "\n",
    "    def forward(self, up_p, x_p):\n",
    "        up_p = self.tr_conv(up_p)\n",
    "       # pdb.set_trace()\n",
    "        print(\"x_p\",x_p.size())\n",
    "        x_p = self.x_conv(x_p)\n",
    "        cat_p = torch.cat([up_p,x_p], dim=1)\n",
    "        return self.bn(F.relu(cat_p))  \n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Unet34(nn.Module):\n",
    "    def __init__(self, rn):\n",
    "        super().__init__()\n",
    "        self.rn = rn\n",
    "        self.sfs = [SaveFeatures(rn[i]) for i in [2,4,5,6]]\n",
    "        self.up1 = UnetBlock(512,260,256)\n",
    "        self.up2 = UnetBlock(256,132,128)\n",
    "        self.up3 = UnetBlock(128,64,128)\n",
    "        self.up4 = UnetBlock(128,64,128)\n",
    "        self.up5 = nn.ConvTranspose2d(128,1, 2, stride=2)                   \n",
    "       # pdb.set_trace()\n",
    "        self.up6 = nn.Conv2d(1,1,kernel_size=1,padding=0)\n",
    "        self.upn = nn.ReLU(nn.ConvTranspose2d(512,1,1,stride =1))\n",
    "        self.logit = nn.Sequential(nn.Conv2d(641,64,kernel_size=3,padding=1),\n",
    "                                   nn.ReLU(inplace=True),\n",
    "                                   nn.ConvTranspose2d(64,1,kernel_size=1,padding=0))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        x = F.relu(self.rn(x))\n",
    "        #print(\"size of x\", type(x))\n",
    "        print(\"sfs 3 \",self.sfs[3].features.size())\n",
    "        xtt = []\n",
    "        xt = torch.split(self.sfs[3].features,64,dim=1)\n",
    "      #  print(\"xt type\", xt[0].size())\n",
    "       # xtt = F.conv2d(self.sfs[0].features,xt[0].contiguous(),padding=1,stride=4)\n",
    "        for i in range(0,len(xt)):  \n",
    "            xt_new= torch.sum(xt[i],0)\n",
    "            xt_new1 = xt_new.unsqueeze(0)\n",
    "            xt1 = F.relu(F.conv2d(self.sfs[0].features,xt_new1.contiguous(),stride=8),inplace=True)\n",
    "         #   print(\"xt1\", xt1.size())\n",
    "            xtt.append(xt1)\n",
    "        xtt_1 = torch.cat([xtt[0],xtt[1],xtt[2],xtt[3]],1)\n",
    "        xtt_2 = torch.cat((xtt_1, self.sfs[3].features),1)\n",
    "        x3 = self.up1(x, xtt_2)\n",
    "        #print(\"sfs3 deatures\", self.sfs[3].features.size())\n",
    "        print(\"x-afet up1\",x3.size())\n",
    "        xtt = []\n",
    "        for i in range(0,len(xt)):  \n",
    "            xt_new= torch.sum(xt[i],0)\n",
    "            xt_new1 = xt_new.unsqueeze(0)\n",
    "            xt1 = F.relu(F.conv2d(self.sfs[1].features,xt_new1.contiguous(),stride=2,padding=3),inplace=True)\n",
    "        #    print(\"xt 1\", xt1.size())\n",
    "            xtt.append(xt1)\n",
    "        xtt_3 = torch.cat([xtt[0],xtt[1],xtt[2],xtt[3]],1)\n",
    "        print(\"xtt 3\", xtt_3[0].size())\n",
    "#         print(\"xttt 3\", xtt_3.size())\n",
    "        xtt_4 = torch.cat((xtt_3, self.sfs[2].features),1)\n",
    "        x2 = self.up2(x3, xtt_4)\n",
    "        xtt = []\n",
    "        xt = torch.split(self.sfs[3].features,128,dim=1)\n",
    "        for i in range(0,len(xt)):  \n",
    "            xt_new= torch.sum(xt[i],0)\n",
    "            xt_new1 = xt_new.unsqueeze(0)\n",
    "            print(\"sfs 2\", self.sfs[2].features.size())\n",
    "            print(\"xt_new1\", xt_new1.size())\n",
    "            xt1 = F.relu(F.conv2d(self.sfs[2].features,xt_new1.contiguous(),padding=3),inplace=True)\n",
    "            xt1 = F.upsample(xt1,size=(16,16),mode='bilinear')\n",
    "            print(\"xt 1\", xt1.size())\n",
    "            xtt.append(xt1)\n",
    "        xtt_5 = torch.cat([xtt[0],xtt[1]],1)\n",
    "        print(\"xtt \", xtt_5[0].size())\n",
    "#         print(\"xttt 3\", xtt_3.size())\n",
    "        xtt_6 = torch.cat((xtt_5, self.sfs[2].features),1)\n",
    "        x1 = self.up3(x2, self.sfs[1].features)\n",
    "        x0 = self.up4(x1, self.sfs[0].features)  \n",
    "        x00 = self.up5(x0)\n",
    "       # print(\"x00\", x00.size())\n",
    "        concated = torch.cat((x00,\n",
    "                        F.upsample(x0, scale_factor=2,mode=\"bilinear\"),\n",
    "                        F.upsample(x1, scale_factor=4,mode=\"bilinear\"),\n",
    "                        F.upsample(x2, scale_factor=8,mode=\"bilinear\"),\n",
    "                        F.upsample(x3, scale_factor=16,mode=\"bilinear\")\n",
    "                      ),1)\n",
    "        print(\"after concat\", concated.size())\n",
    "        x = self.logit(concated)\n",
    "        x = F.dropout(x,p=0.75)\n",
    "        return x[:,0]\n",
    "    \n",
    "    def close(self):\n",
    "        for sf in self.sfs: sf.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetModel():\n",
    "    def __init__(self,model,name='unet'):\n",
    "        self.model,self.name = model,name\n",
    "\n",
    "    def get_layer_groups(self, precompute):\n",
    "        lgs = list(split_by_idxs(children(self.model.rn), [lr_cut]))\n",
    "        return lgs + [children(self.model)[1:]]\n",
    "    \n",
    "\n",
    "class dice_loss(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(dice_loss, self).__init__()\n",
    "\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        input = torch.sigmoid(input)\n",
    "        smooth = 1.\n",
    "\n",
    "        iflat = input.view(-1)\n",
    "        tflat = target.view(-1)\n",
    "        intersection = (iflat * tflat).sum()\n",
    "\n",
    "        return 1 - ((2. * intersection + smooth) /\n",
    "                  (iflat.sum() + tflat.sum() + smooth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4285762d0b34409916dc92eb1f33ca5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, description='Epoch', max=80), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch      trn_loss   val_loss   <lambda>   IOU                                                                        \n",
      "    0      2.000958   2.000146   0.755723   0.194473  \n",
      "    1      2.000472   2.001492   0.755723   0.195664                                                                   \n",
      "    2      2.000451   2.00017    0.755723   0.195664                                                                   \n",
      "    3      2.000407   2.000512   0.755723   0.0                                                                        \n",
      "    4      2.000404   2.000125   0.755723   0.0                                                                        \n",
      "    5      2.000329   2.000069   0.755723   0.195664                                                                   \n",
      "    6      2.000263   2.000458   0.755723   0.195664                                                                   \n",
      "    7      2.000299   2.000432   0.755723   0.0                                                                        \n",
      "    8      2.000352   2.000226   0.755723   0.195664                                                                   \n",
      "    9      2.000364   2.000076   0.755723   0.0                                                                        \n",
      "    10     2.00029    2.000606   0.755723   0.0                                                                        \n",
      "    11     2.000433   2.000372   0.755723   0.0                                                                        \n",
      "    12     2.000551   2.000381   0.755723   0.0                                                                        \n",
      "    13     2.000423   2.000422   0.755723   0.0                                                                        \n",
      "    14     2.000398   2.000656   0.755723   0.195664                                                                   \n",
      "    15     2.000319   2.000282   0.755723   0.0                                                                        \n",
      "    16     2.000212   2.000206   0.755723   0.0                                                                        \n",
      "    17     2.000246   2.000055   0.755723   0.195664                                                                   \n",
      "    18     2.000564   2.00005    0.755723   0.0                                                                        \n",
      "    19     2.000323   2.000265   0.755723   0.0                                                                        \n",
      "    20     2.000433   2.000613   0.755723   0.0                                                                        \n",
      "    21     2.000347   2.000084   0.755723   0.0                                                                        \n",
      "    22     2.000512   2.000155   0.755723   0.0                                                                        \n",
      "    23     2.000281   2.000199   0.755723   0.0                                                                        \n",
      "    24     2.000438   2.000182   0.755723   0.0                                                                        \n",
      "    25     2.000229   2.000185   0.755723   0.0                                                                        \n",
      "    26     2.000236   2.000142   0.755723   0.0                                                                        \n",
      "    27     2.000289   2.000358   0.755723   0.0                                                                        \n",
      "    28     2.000386   2.000849   0.755723   0.195664                                                                   \n",
      "    29     2.000295   2.000185   0.755723   0.195664                                                                   \n",
      "    30     2.000431   2.000075   0.755723   0.0                                                                        \n",
      "    31     2.000316   2.000014   0.755723   0.195664                                                                   \n",
      "    32     2.000319   2.000154   0.755723   0.0                                                                        \n",
      "    33     2.000415   2.000059   0.755723   0.0                                                                        \n",
      "    34     2.000297   2.000227   0.755723   0.0                                                                        \n",
      "    35     2.000531   2.000275   0.755723   0.0                                                                        \n",
      "    36     2.000291   2.000405   0.755723   0.0                                                                        \n",
      "    37     2.000383   2.000096   0.755723   0.0                                                                        \n",
      "    38     2.000217   2.000041   0.755723   0.0                                                                        \n",
      "    39     2.000174   2.000187   0.755723   0.0                                                                        \n",
      "    40     2.000309   2.000337   0.755723   0.0                                                                        \n",
      "    41     2.000293   2.000011   0.755723   0.195664                                                                   \n",
      "    42     2.000472   2.000278   0.755723   0.0                                                                        \n",
      "    43     2.000444   2.000127   0.755723   0.0                                                                        \n",
      "    44     2.00036    2.000086   0.755723   0.0                                                                        \n",
      "    45     2.00032    2.000432   0.755723   0.195664                                                                   \n",
      "    46     2.000365   2.000085   0.755723   0.0                                                                        \n",
      "    47     2.000224   2.000229   0.755723   0.0                                                                        \n",
      "    48     2.000187   2.000084   0.755723   0.0                                                                        \n",
      "    49     2.000426   2.000281   0.755723   0.0                                                                        \n",
      "    50     2.000371   2.000378   0.755723   0.195664                                                                   \n",
      "    51     2.000278   2.000279   0.755723   0.0                                                                        \n",
      "    52     2.000203   2.000182   0.755723   0.0                                                                        \n",
      "    53     2.00028    2.000422   0.755723   0.195664                                                                   \n",
      "    54     2.00023    2.000178   0.755723   0.0                                                                        \n",
      "    55     2.000212   2.000044   0.755723   0.0                                                                        \n",
      "    56     2.000357   2.000404   0.755723   0.0                                                                        \n",
      "    57     2.000303   2.000294   0.755723   0.195664                                                                   \n",
      "    58     2.000291   2.000507   0.755723   0.195664                                                                   \n",
      "    59     2.000538   2.00027    0.755723   0.0                                                                        \n",
      "    60     2.000537   2.000155   0.755723   0.0                                                                        \n",
      "    61     2.000324   2.000167   0.755723   0.195664                                                                   \n",
      "    62     2.000308   2.000939   0.755723   0.195664                                                                   \n",
      "    63     2.000384   2.00057    0.755723   0.195664                                                                   \n",
      "    64     2.000368   2.000221   0.755723   0.0                                                                        \n",
      "    65     2.000247   2.0        0.755723   0.0                                                                        \n",
      "    66     2.000254   2.000053   0.755723   0.195664                                                                   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    67     2.000283   2.000698   0.755723   0.195664                                                                   \n",
      "    68     2.000254   2.000148   0.755723   0.195664                                                                   \n",
      "    69     2.000212   2.000363   0.755723   0.0                                                                        \n",
      "    70     2.000196   2.000176   0.755723   0.195664                                                                   \n",
      "    71     2.000226   2.000265   0.755723   0.0                                                                        \n",
      "    72     2.000291   2.000071   0.755723   0.0                                                                        \n",
      "    73     2.00043    2.000223   0.755723   0.0                                                                        \n",
      "    74     2.000382   2.000033   0.755723   0.0                                                                        \n",
      "    75     2.000295   2.000032   0.755723   0.0                                                                        \n",
      "    76     2.00042    2.000082   0.755723   0.195664                                                                   \n",
      "    77     2.000265   2.000044   0.755723   0.0                                                                        \n",
      "    78     2.000262   2.000429   0.755723   0.195664                                                                   \n",
      "    79     2.00033    2.000278   0.755723   0.0                                                                        \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([2.00028]), 0.7557226634025573, 0.0]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class double_conv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(double_conv, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,\n",
    "                              stride=stride, padding=padding),\n",
    "                  #  nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size,\n",
    "                              stride=stride, padding=padding),\n",
    "                   # nn.BatchNorm2d(out_channels),\n",
    "                    nn.ReLU(inplace=True))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        return x\n",
    "        \n",
    "start_fm = 16\n",
    "\n",
    "class Unet(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Unet, self).__init__()\n",
    "        \n",
    "        # Input 128x128x1\n",
    "        \n",
    "        #Contracting Path\n",
    "        \n",
    "        #(Double) Convolution 1        \n",
    "        self.double_conv1 = double_conv(3, start_fm, 3, 1, 1)\n",
    "        #Max Pooling 1\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution 2\n",
    "        self.double_conv2 = double_conv(start_fm, start_fm * 2, 3, 1, 1)\n",
    "        #Max Pooling 2\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution 3\n",
    "        self.double_conv3 = double_conv(start_fm * 2, start_fm * 4, 3, 1, 1)\n",
    "        #Max Pooling 3\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution 4\n",
    "        self.double_conv4 = double_conv(start_fm * 4, start_fm * 8, 3, 1, 1)\n",
    "        #Max Pooling 4\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2)\n",
    "        \n",
    "        #Convolution 5\n",
    "        self.double_conv5 = double_conv(start_fm * 8, start_fm * 16, 3, 1, 1)\n",
    "        \n",
    "        #Transposed Convolution 4\n",
    "        self.t_conv4 = nn.ConvTranspose2d(start_fm * 16, start_fm * 8, 2, 2)\n",
    "        # Expanding Path Convolution 4 \n",
    "        self.ex_double_conv4 = double_conv(start_fm * 16, start_fm * 8, 3, 1, 1)\n",
    "        \n",
    "        #Transposed Convolution 3\n",
    "        self.t_conv3 = nn.ConvTranspose2d(start_fm * 8, start_fm * 4, 2, 2)\n",
    "        #Convolution 3\n",
    "        self.ex_double_conv3 = double_conv(start_fm * 8, start_fm * 4, 3, 1, 1)\n",
    "        \n",
    "        #Transposed Convolution 2\n",
    "        self.t_conv2 = nn.ConvTranspose2d(start_fm * 4, start_fm * 2, 2, 2)\n",
    "        #Convolution 2\n",
    "        self.ex_double_conv2 = double_conv(start_fm * 4, start_fm * 2, 3, 1, 1)\n",
    "        \n",
    "        #Transposed Convolution 1\n",
    "        self.t_conv1 = nn.ConvTranspose2d(start_fm * 2, start_fm, 2, 2)\n",
    "        #Convolution 1\n",
    "        self.ex_double_conv1 = double_conv(start_fm * 2, start_fm, 3, 1, 1)\n",
    "        \n",
    "        # One by One Conv\n",
    "        self.one_by_one = nn.Conv2d(start_fm, 1, 1, 1, 0)\n",
    "        #self.final_act = nn.Sigmoid()\n",
    "        \n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # Contracting Path\n",
    "        conv1 = self.double_conv1(inputs)\n",
    "        maxpool1 = self.maxpool1(conv1)\n",
    "\n",
    "        conv2 = self.double_conv2(maxpool1)\n",
    "        maxpool2 = self.maxpool2(conv2)\n",
    "\n",
    "        conv3 = self.double_conv3(maxpool2)\n",
    "        maxpool3 = self.maxpool3(conv3)\n",
    "\n",
    "        conv4 = self.double_conv4(maxpool3)\n",
    "        maxpool4 = self.maxpool4(conv4)\n",
    "            \n",
    "        # Bottom\n",
    "        conv5 = self.double_conv5(maxpool4)\n",
    "        \n",
    "        # Expanding Path\n",
    "        t_conv4 = self.t_conv4(conv5)\n",
    "        cat4 = torch.cat([conv4 ,t_conv4], 1)\n",
    "        ex_conv4 = self.ex_double_conv4(cat4)\n",
    "        \n",
    "        t_conv3 = self.t_conv3(ex_conv4)\n",
    "        cat3 = torch.cat([conv3 ,t_conv3], 1)\n",
    "        ex_conv3 = self.ex_double_conv3(cat3)\n",
    "\n",
    "        t_conv2 = self.t_conv2(ex_conv3)\n",
    "        cat2 = torch.cat([conv2 ,t_conv2], 1)\n",
    "        ex_conv2 = self.ex_double_conv2(cat2)\n",
    "        \n",
    "        t_conv1 = self.t_conv1(ex_conv2)\n",
    "        cat1 = torch.cat([conv1 ,t_conv1], 1)\n",
    "        ex_conv1 = self.ex_double_conv1(cat1)\n",
    "        \n",
    "        one_by_one = self.one_by_one(ex_conv1)\n",
    "       # print(one_by_one[:,0:1].shape)\n",
    "        return one_by_one[:,0:1]\n",
    "    \n",
    "model = Unet()\n",
    "model.cuda();\n",
    "opt = optim.Adam(model.parameters(), 0.005)\n",
    "model.parameters\n",
    "crit=cust_loss()\n",
    "metrics=[accuracy_thresh(0.5),IOU]\n",
    "fit(model, md, 80, opt, crit, metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open(\"test\",'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.load(open(\"test\",'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "m_base = get_base()\n",
    "m = to_gpu(Unet34(m_base))\n",
    "models = UnetModel(m)\n",
    "learn = ConvLearner(md, models)\n",
    "learn.opt_fn=optim.Adam\n",
    "learn.crit=cust_loss()\n",
    "learn.metrics=[accuracy_thresh(0.5),IOU]\n",
    "# learn.crit\n",
    "\n",
    "# learn.lr_find(0.001,0.1)\n",
    "# learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs,1,cycle_len=50,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('128urn-logit')\n",
    "learn.load('128urn-logit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('128urn-logit_tmp')\n",
    "learn.load('128urn-logit_tmp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs,10,wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs,50,wds=wd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = ConvLearner(md, models)\n",
    "learn.opt_fn=optim.Adam\n",
    "learn.crit=cust_loss()\n",
    "learn.metrics=[accuracy_thresh(0.5),IOU]\n",
    "learn.crit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learn.save_fc1()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[o.features.size() for o in m.sfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.freeze_to(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "learn.lr_find(0.008,0.1)\n",
    "learn.sched.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=8e-2\n",
    "wd=1e-7\n",
    "lrs = np.array([lr/100,lr/10,lr])\n",
    "lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lr,1,cycle_len=10,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del learn\n",
    "del models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "learn.fit(lr,1,cycle_len=10,use_clr=(5,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs,1,wds=wd,cycle_len=50,use_clr=(20,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=1e-1\n",
    "wd=1e-4\n",
    "\n",
    "lrs = np.array([lr/100,lr/10,lr])\n",
    "lr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('128urn-k1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.load('128urn-k1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.unfreeze()\n",
    "learn.bn_freeze(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('128urn-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.load('128urn-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, cycle_len=30,use_clr=(10,30))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learn.save('128urn-unfreeze-3')\n",
    "learn.load('128urn-unfreeze-3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "learn.fit(lrs, 1, cycle_len=30,use_clr=(10,30))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "toc": {
   "colors": {
    "hover_highlight": "#DAA520",
    "navigate_num": "#000000",
    "navigate_text": "#333333",
    "running_highlight": "#FF0000",
    "selected_highlight": "#FFD700",
    "sidebar_border": "#EEEEEE",
    "wrapper_background": "#FFFFFF"
   },
   "moveMenuLeft": true,
   "nav_menu": {
    "height": "86px",
    "width": "252px"
   },
   "navigate_menu": true,
   "number_sections": true,
   "sideBar": true,
   "threshold": 4,
   "toc_cell": false,
   "toc_section_display": "block",
   "toc_window_display": false,
   "widenNotebook": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
